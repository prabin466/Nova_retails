{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Spark Setup\n",
    "We create a SparkSession to process batch CSvs and micro-batch inventory streams.\n",
    "The app name and master configuration are from \"config.yaml\"."
   ],
   "id": "a09aa5baf070d3cb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-19T11:45:32.661408Z",
     "start_time": "2025-09-19T11:45:32.659097Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "\n",
    "project_root = Path.cwd().parent"
   ],
   "id": "54e8cb136e57d641",
   "outputs": [],
   "execution_count": 99
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-19T11:45:32.677954Z",
     "start_time": "2025-09-19T11:45:32.671127Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import yaml\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Load config.yaml\n",
    "with open(\"../config.yaml\", 'r') as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    " # Initialize Spark Session\n",
    "spark = (SparkSession.builder\n",
    "    .appName(cfg[\"spark\"][\"app_name\"])\n",
    "    .master(cfg[\"spark\"][\"master\"])\n",
    "    .getOrCreate())\n",
    "spark"
   ],
   "id": "633b96bf82fe4b74",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f8e239c5390>"
      ],
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.255.255.254:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>NovaRetailMongoPipeline</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 100
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Data Ingestion\n",
    "We are loading batch load of historical sales and customers data and a stimulated mini-stream files drop for inventory sensor events."
   ],
   "id": "c004ee281f211175"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-19T11:45:32.727784Z",
     "start_time": "2025-09-19T11:45:32.725378Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Configuring path for global use\n",
    "sales_path = project_root/ \"data\" / \"raw\" /\"sales_data.csv\"\n",
    "customers_path = project_root/ \"data\" / \"raw\" /\"customers_data.csv\"\n",
    "inventory_path = project_root/ \"data\" / \"raw\" /\"inventory_stream\""
   ],
   "id": "7a3cfc46e59a85c1",
   "outputs": [],
   "execution_count": 101
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-19T11:45:32.982682Z",
     "start_time": "2025-09-19T11:45:32.772435Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sales_df = spark.read.csv(str(sales_path), header=True, inferSchema=True)\n",
    "customers_df = spark.read.csv(str(customers_path), header=True, inferSchema=True)\n",
    "\n",
    "sales_df.show(5)\n",
    "customers_df.show(5)\n"
   ],
   "id": "d0109c6abadb04d0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+------+--------+-----------+--------+-------+\n",
      "|txn_id|    date|sku_id|store_id|customer_id|quantity|  price|\n",
      "+------+--------+------+--------+-----------+--------+-------+\n",
      "|T13899|20250830|  P001|    X001|       C003|      23|2240.29|\n",
      "|T87892|20250830|  P001|    X001|       C002|       6| 613.95|\n",
      "|T35545|20250830|  P001|    X001|       C002|       6| 634.08|\n",
      "|T96496|20250830|  P001|    X001|       C002|      19|1815.12|\n",
      "|T72236|20250830|  P001|    X001|       C004|      14|1388.67|\n",
      "+------+--------+------+--------+-----------+--------+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----------+-------------+------------+-----------+\n",
      "|customer_id|last_purchase|total_orders|total_spend|\n",
      "+-----------+-------------+------------+-----------+\n",
      "|       C001|     20250822|          63|       6174|\n",
      "|       C002|     20250820|          27|       3240|\n",
      "|       C003|     20250828|          53|       5777|\n",
      "|       C004|     20250825|          43|       7740|\n",
      "+-----------+-------------+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 102
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We have to define schema for Spark as Spark would not know the schema for inventory as we will be uploading inventory stream batch files incrementally.",
   "id": "6c0be4ffa2ad6be6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-19T11:45:33.005977Z",
     "start_time": "2025-09-19T11:45:32.997109Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# Define Schema\n",
    "inventory_schema = StructType([\n",
    "    StructField(\"timestamp\", StringType(), True),\n",
    "    StructField(\"store_id\", StringType(), True),\n",
    "    StructField(\"sku_id\", StringType(), True),\n",
    "    StructField(\"on_stock\", IntegerType(), True),\n",
    "])\n",
    "\n",
    "# Read streaming JSON with mini-batch drops\n",
    "inventory_df = spark.readStream \\\n",
    "    .format(\"json\") \\\n",
    "    .schema(inventory_schema) \\\n",
    "    .option(\"header\", \"true\")  \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"maxFilesPerTrigger\",1) \\\n",
    "    .load(str(inventory_path))\n",
    "\n",
    "inventory_df.printSchema()\n"
   ],
   "id": "b13cd1f284467734",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- store_id: string (nullable = true)\n",
      " |-- sku_id: string (nullable = true)\n",
      " |-- on_stock: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 103
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Processing Layer - Curate the Data\n",
    "### Daily item-store sales\n",
    "It tells us how many items were in stock for each SKU at each store per day. This is the foundation for demand calculations.\n",
    "### Moving Average Demand\n",
    "It helps us smooth out fluctuations and detect trends. For streaming , we can compute it using window functions.\n",
    "### Stock-Out Risk Signal\n",
    "It shows risk signals for Stock-Out.\n",
    "### RFM Analysis On Customers\n",
    "It involves Recency which means how recently a customer purchased, Frequency means how often a customer purchased and Monetary means total spend.\n"
   ],
   "id": "d878b1face15d23f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Daily Item-Store Sales And Moving Average OF Quantity",
   "id": "84bbf6c83872e35d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-19T11:45:33.175731Z",
     "start_time": "2025-09-19T11:45:33.056884Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Aggregate sales at store, date and sku level\n",
    "sales_df = sales_df.withColumn(\"date\", f.to_date(f.col(\"date\").cast(\"string\"), \"yyyyMMdd\"))\n",
    "daily_item_store = sales_df.groupby(\"date\", \"store_id\", \"sku_id\") \\\n",
    "    .agg(\n",
    "    f.sum(\"quantity\").alias(\"total_qty\"),\n",
    "    f.round(f.sum(\"price\"),2).alias(\"total_sales\"),\n",
    "    f.round(f.avg(\"price\"), 2).alias(\"avg_price\"),\n",
    "    f.max(\"price\").alias(\"max_price\"),\n",
    "    f.min(\"price\").alias(\"min_price\")\n",
    ")\n",
    "\n",
    "# Moving Average of Quantity/Demand\n",
    "window_spec = Window.partitionBy(\"store_id\", \"sku_id\").orderBy(\"date\").rowsBetween(-6,0)\n",
    "daily_item_store = daily_item_store.withColumn(\"moving_avg_qty\", f.avg(\"total_qty\").over(window_spec))\n",
    "\n",
    "# Performance optimizations\n",
    "\n",
    "# Partition distributes DataFrame across Spark partitions based on store_id and sku_id. This helps reduce shuffling when performing joins or window operation because rows with the same store and SKU end up in the same partition. It improves performance for subsequent operations like window functions or aggregations.\n",
    "daily_item_store = daily_item_store.repartition(\"store_id\", \"sku_id\")\n",
    "\n",
    "# If the following DataFrame is used multiple times, Spark doesn't need to recompute it from scratch each time. This speeds up repeated operations and avoids redundant computations.\n",
    "daily_item_store = daily_item_store.cache()\n",
    "\n",
    "daily_item_store.show(5)"
   ],
   "id": "70da52bee5502af5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+------+---------+-----------+---------+---------+---------+------------------+\n",
      "|      date|store_id|sku_id|total_qty|total_sales|avg_price|max_price|min_price|    moving_avg_qty|\n",
      "+----------+--------+------+---------+-----------+---------+---------+---------+------------------+\n",
      "|2025-08-30|    X002|  P002|      224|   44874.94|   4986.1|  9149.04|  1695.15|             224.0|\n",
      "|2025-08-31|    X002|  P002|      185|   37471.49|  5353.07|  8159.79|  1334.84|             204.5|\n",
      "|2025-09-01|    X002|  P002|      234|   46675.81|  4667.58|   8398.4|  2164.03|214.33333333333334|\n",
      "|2025-09-02|    X002|  P002|      242|   48352.79|   6044.1|  9510.97|  1076.87|            221.25|\n",
      "|2025-09-03|    X002|  P002|      327|   66061.94|  7340.22| 10158.55|  3053.65|             242.4|\n",
      "+----------+--------+------+---------+-----------+---------+---------+---------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/19 21:45:33 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    }
   ],
   "execution_count": 104
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Stock-Out Risk Signal",
   "id": "e0177f14fa8a5dbf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-19T11:46:48.084979Z",
     "start_time": "2025-09-19T11:46:47.530280Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Extracting date from timestamp for joiing\n",
    "spark.stop()"
   ],
   "id": "772267ead31430ee",
   "outputs": [],
   "execution_count": 106
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b37432db2b737220"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
