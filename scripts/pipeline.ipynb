{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Spark Setup\n",
    "We create a SparkSession to process batch CSvs and micro-batch inventory streams.\n",
    "The app name and master configuration are from \"config.yaml\"."
   ],
   "id": "a09aa5baf070d3cb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-18T14:27:46.897946Z",
     "start_time": "2025-09-18T14:27:46.894182Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path.cwd().parent"
   ],
   "id": "54e8cb136e57d641",
   "outputs": [],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-18T14:27:46.959586Z",
     "start_time": "2025-09-18T14:27:46.945057Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import yaml\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Load config.yaml\n",
    "with open(\"../config.yaml\", 'r') as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    " # Initialize Spark Session\n",
    "spark = (SparkSession.builder\n",
    "    .appName(cfg[\"spark\"][\"app_name\"])\n",
    "    .master(cfg[\"spark\"][\"master\"])\n",
    "    .getOrCreate())\n",
    "spark"
   ],
   "id": "633b96bf82fe4b74",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f219fa12770>"
      ],
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.255.255.254:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>NovaRetailMongoPipeline</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Data Ingestion\n",
    "We are loading batch load of historical sales and customers data and a stimulated mini-stream files drop for inventory sensor events."
   ],
   "id": "c004ee281f211175"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-18T14:27:47.013218Z",
     "start_time": "2025-09-18T14:27:47.009193Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Configuring path for global use\n",
    "sales_path = project_root/ \"data\" / \"raw\" /\"\"\n",
    "customers_path = project_root/ \"data\" / \"raw\" /\"\"\n",
    "inventory_path = project_root/ \"data\" / \"raw\" /\"\""
   ],
   "id": "7a3cfc46e59a85c1",
   "outputs": [],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-18T14:27:47.356672Z",
     "start_time": "2025-09-18T14:27:47.074076Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sales_df = spark.read.csv(str(sales_path), header=True, inferSchema=True)\n",
    "customers_df = spark.read.csv(str(customers_path), header=True, inferSchema=True)\n",
    "\n",
    "sales_df.show(5)\n",
    "customers_df.show(5)\n"
   ],
   "id": "d0109c6abadb04d0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+------+-----------+--------+-----+\n",
      "|txn_id|    date|sku_id|customer_id|quantity|price|\n",
      "+------+--------+------+-----------+--------+-----+\n",
      "| T3650|20250830|  P001|       C002|      17| 1700|\n",
      "| T4366|20250830|  P002|       C004|      10| 2000|\n",
      "| T7417|20250830|  P003|       C002|      20| 6000|\n",
      "| T1662|20250830|  P001|       C003|      32| 3200|\n",
      "| T3343|20250830|  P002|       C004|      40| 8000|\n",
      "+------+--------+------+-----------+--------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+------+--------+------+-----------+--------+-----+\n",
      "|txn_id|    date|sku_id|customer_id|quantity|price|\n",
      "+------+--------+------+-----------+--------+-----+\n",
      "| T3650|20250830|  P001|       C002|      17| 1700|\n",
      "| T4366|20250830|  P002|       C004|      10| 2000|\n",
      "| T7417|20250830|  P003|       C002|      20| 6000|\n",
      "| T1662|20250830|  P001|       C003|      32| 3200|\n",
      "| T3343|20250830|  P002|       C004|      40| 8000|\n",
      "+------+--------+------+-----------+--------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We have to define schema for Spark as Spark would not know the schema for inventory as we will be uploading inventory stream batch files incrementally.",
   "id": "6c0be4ffa2ad6be6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-18T14:27:47.371627Z",
     "start_time": "2025-09-18T14:27:47.362189Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# Define Schema\n",
    "inventory_schema = StructType([\n",
    "    StructField(\"timestamp\", StringType(), True),\n",
    "    StructField(\"store_id\", StringType(), True),\n",
    "    StructField(\"sku_id\", StringType(), True),\n",
    "    StructField(\"on_stock\", IntegerType(), True),\n",
    "])\n",
    "\n",
    "# Read streaming JSON with mini-batch drops\n",
    "inventory_df = spark.readStream \\\n",
    "    .format(\"json\") \\\n",
    "    .schema(inventory_schema) \\\n",
    "    .option(\"header\", \"true\")  \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"maxFilesPerTrigger\",1) \\\n",
    "    .load(str(inventory_path))\n",
    "\n",
    "inventory_df.printSchema()\n"
   ],
   "id": "b13cd1f284467734",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- store_id: string (nullable = true)\n",
      " |-- sku_id: string (nullable = true)\n",
      " |-- on_stock: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Processing Layer - Curate the Data\n",
    "### Daily item-store sales\n",
    "It tells us how many items were in stock for each SKU at each store per day. This is the foundation for demand calculations.\n",
    "### Moving Average Demand\n",
    "It helps us smooth out fluctuations and detect trends. For streaming , we can compute it using window functions.\n",
    "### Stock-Out Risk Signal\n",
    "It shows risk signals for Stock-Out.\n",
    "### RFM Analysis On Customers\n",
    "It involves Recency which means how recently a customer purchased, Frequency means how often a customer purchased and Monetary means total spend.\n"
   ],
   "id": "d878b1face15d23f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-18T14:33:11.899562Z",
     "start_time": "2025-09-18T14:33:11.786691Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import to_date, col, sum as spark_sum\n",
    "\n",
    "spark.stop()"
   ],
   "id": "70da52bee5502af5",
   "outputs": [],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-18T14:27:47.474403Z",
     "start_time": "2025-09-18T14:27:47.472739Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "772267ead31430ee",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
