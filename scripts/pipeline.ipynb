{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Spark Setup\n",
    "We create a SparkSession to process batch CSvs and micro-batch inventory streams.\n",
    "The app name and master configuration are from \"config.yaml\"."
   ],
   "id": "a09aa5baf070d3cb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T10:26:31.470457Z",
     "start_time": "2025-09-20T10:26:31.468072Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "\n",
    "project_root = Path.cwd().parent"
   ],
   "id": "54e8cb136e57d641",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T10:26:31.486496Z",
     "start_time": "2025-09-20T10:26:31.478166Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import yaml\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "# Load config.yaml\n",
    "with open(\"../config.yaml\", 'r') as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    " # Initialize Spark Session\n",
    "spark = (SparkSession.builder\n",
    "    .appName(cfg[\"spark\"][\"app_name\"])\n",
    "    .master(cfg[\"spark\"][\"master\"])\n",
    "    .config(\"spark.mongodb.write.connection.uri\", \"mongodb://localhost:27017/retail_db\")\n",
    "    .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:10.5.0\")\n",
    "    .getOrCreate())\n",
    "spark"
   ],
   "id": "633b96bf82fe4b74",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fd432d84910>"
      ],
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.255.255.254:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>NovaRetailMongoPipeline</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T10:26:31.553555Z",
     "start_time": "2025-09-20T10:26:31.538674Z"
    }
   },
   "cell_type": "code",
   "source": "print(spark.sparkContext.getConf().get(\"spark.jars\"))",
   "id": "a830290f840f0352",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file:///home/prabin/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.12-10.5.0.jar,file:///home/prabin/.ivy2/jars/org.mongodb_mongodb-driver-sync-5.1.4.jar,file:///home/prabin/.ivy2/jars/org.mongodb_bson-5.1.4.jar,file:///home/prabin/.ivy2/jars/org.mongodb_mongodb-driver-core-5.1.4.jar,file:///home/prabin/.ivy2/jars/org.mongodb_bson-record-codec-5.1.4.jar\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Data Ingestion\n",
    "We are loading batch load of historical sales and customers data and a stimulated mini-stream files drop for inventory sensor events."
   ],
   "id": "c004ee281f211175"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T10:26:31.609163Z",
     "start_time": "2025-09-20T10:26:31.606449Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Configuring path for global use\n",
    "sales_path = project_root/ \"data\" / \"raw\" /\"sales_data.csv\"\n",
    "customers_path = project_root/ \"data\" / \"raw\" /\"customers_data.csv\"\n",
    "inventory_path = project_root/ \"data\" / \"raw\" /\"inventory_stream\""
   ],
   "id": "7a3cfc46e59a85c1",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T10:26:32.060602Z",
     "start_time": "2025-09-20T10:26:31.667658Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sales_df = spark.read.csv(str(sales_path), header=True, inferSchema=True)\n",
    "customers_df = spark.read.csv(str(customers_path), header=True, inferSchema=True)\n",
    "\n",
    "sales_df.show(5)\n",
    "customers_df.show(5)\n"
   ],
   "id": "d0109c6abadb04d0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+------+--------+-----------+--------+-------+\n",
      "|txn_id|    date|sku_id|store_id|customer_id|quantity|  price|\n",
      "+------+--------+------+--------+-----------+--------+-------+\n",
      "|  T522|20250830|  P001|    X001|       C001|      18|1719.85|\n",
      "|  T618|20250830|  P001|    X001|       C001|      11|1192.01|\n",
      "|  T424|20250830|  P001|    X001|       C004|      26|2613.83|\n",
      "|  T565|20250830|  P001|    X001|       C001|      50|5315.88|\n",
      "|  T686|20250830|  P001|    X001|       C001|      45|4468.59|\n",
      "+------+--------+------+--------+-----------+--------+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----------+-------------+------------+-----------+\n",
      "|customer_id|last_purchase|total_orders|total_spend|\n",
      "+-----------+-------------+------------+-----------+\n",
      "|       C001|     20250828|          36|       6516|\n",
      "|       C002|     20250826|          27|       5292|\n",
      "|       C003|     20250823|          26|       2574|\n",
      "|       C004|     20250828|          37|       2331|\n",
      "+-----------+-------------+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We have to define schema for Spark as Spark would not know the schema for inventory as we will be uploading inventory stream batch files incrementally.",
   "id": "6c0be4ffa2ad6be6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T10:26:32.090538Z",
     "start_time": "2025-09-20T10:26:32.068079Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# Define Schema\n",
    "inventory_schema = StructType([\n",
    "    StructField(\"timestamp\", StringType(), True),\n",
    "    StructField(\"store_id\", StringType(), True),\n",
    "    StructField(\"sku_id\", StringType(), True),\n",
    "    StructField(\"on_stock\", IntegerType(), True),\n",
    "])\n",
    "\n",
    "# Read streaming JSON with mini-batch drops\n",
    "inventory_df = spark.readStream \\\n",
    "    .format(\"json\") \\\n",
    "    .schema(inventory_schema) \\\n",
    "    .option(\"header\", \"true\")  \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"maxFilesPerTrigger\",1) \\\n",
    "    .load(str(inventory_path))\n",
    "\n",
    "inventory_df.printSchema()\n",
    "\n"
   ],
   "id": "b13cd1f284467734",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- store_id: string (nullable = true)\n",
      " |-- sku_id: string (nullable = true)\n",
      " |-- on_stock: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Processing Layer - Curate the Data\n",
    "### Daily item-store sales\n",
    "It tells us how many items were in stock for each SKU at each store per day. This is the foundation for demand calculations.\n",
    "### Moving Average Demand\n",
    "It helps us smooth out fluctuations and detect trends. For streaming , we can compute it using window functions.\n",
    "### Stock-Out Risk Signal\n",
    "It shows risk signals for Stock-Out.\n",
    "### RFM Analysis On Customers\n",
    "It involves Recency which means how recently a customer purchased, Frequency means how often a customer purchased and Monetary means total spend.\n"
   ],
   "id": "d878b1face15d23f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Base function to serve MongoDB",
   "id": "c6065f7c446bb18"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T10:26:32.127600Z",
     "start_time": "2025-09-20T10:26:32.125022Z"
    }
   },
   "cell_type": "code",
   "source": [
    "mongo_uri = cfg[\"mongodb\"][\"uri\"]\n",
    "mongo_db = cfg[\"mongodb\"][\"database\"]\n",
    "collections = cfg[\"mongodb\"][\"collections\"]\n",
    "\n",
    "# Threshold stock quantity min\n",
    "stock_threshold = cfg[\"thresholds\"][\"stock_out_qty\"]"
   ],
   "id": "fd222379fb3e853",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T10:26:32.175564Z",
     "start_time": "2025-09-20T10:26:32.172773Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def write_to_mongo(df, collection_key, mode=\"overwrite\"):\n",
    "    collection_name = collections[collection_key]\n",
    "    df.write \\\n",
    "      .format(\"mongodb\") \\\n",
    "      .mode(mode) \\\n",
    "      .option(\"spark.mongodb.output.uri\", mongo_uri) \\\n",
    "      .option(\"database\", mongo_db) \\\n",
    "      .option(\"collection\", collection_name) \\\n",
    "      .save()\n",
    "\n"
   ],
   "id": "a139628bc943980c",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Daily Item-Store Sales And Moving Average OF Quantity",
   "id": "84bbf6c83872e35d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T10:26:34.777075Z",
     "start_time": "2025-09-20T10:26:32.226537Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Aggregate sales at store, date and sku level\n",
    "sales_df = sales_df.withColumn(\"date\", f.to_date(f.col(\"date\").cast(\"string\"), \"yyyyMMdd\"))\n",
    "daily_item_store = sales_df.groupby(\"date\", \"store_id\", \"sku_id\") \\\n",
    "    .agg(\n",
    "    f.sum(\"quantity\").alias(\"total_qty\"),\n",
    "    f.round(f.sum(\"price\"),2).alias(\"total_sales\"),\n",
    "    f.round(f.avg(\"price\"), 2).alias(\"avg_price\"),\n",
    "    f.max(\"price\").alias(\"max_price\"),\n",
    "    f.min(\"price\").alias(\"min_price\")\n",
    ")\n",
    "\n",
    "# Moving Average of Quantity/Demand\n",
    "window_spec = Window.partitionBy(\"store_id\", \"sku_id\").orderBy(\"date\").rowsBetween(-6,0)\n",
    "daily_item_store = daily_item_store.withColumn(\n",
    "    \"moving_avg_qty\",\n",
    "    f.round(f.avg(\"total_qty\").over(window_spec), 2)  # round to 2 decimals\n",
    ")\n",
    "\n",
    "\n",
    "# Performance optimizations\n",
    "\n",
    "# Partition distributes DataFrame across Spark partitions based on store_id and sku_id. This helps reduce shuffling when performing joins or window operation because rows with the same store and SKU end up in the same partition. It improves performance for subsequent operations like window functions or aggregations.\n",
    "daily_item_store = daily_item_store.repartition(\"store_id\", \"sku_id\")\n",
    "\n",
    "# If the following DataFrame is used multiple times, Spark doesn't need to recompute it from scratch each time. This speeds up repeated operations and avoids redundant computations.\n",
    "daily_item_store = daily_item_store.cache()\n",
    "\n",
    "daily_item_store.show(5)\n",
    "\n",
    "write_to_mongo(daily_item_store, \"sales\") # Pushes curated sales data to MongoDB"
   ],
   "id": "70da52bee5502af5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+------+---------+-----------+---------+---------+---------+--------------+\n",
      "|      date|store_id|sku_id|total_qty|total_sales|avg_price|max_price|min_price|moving_avg_qty|\n",
      "+----------+--------+------+---------+-----------+---------+---------+---------+--------------+\n",
      "|2025-08-30|    X002|  P002|      161|   32226.96|  4028.37|  8834.62|  1279.78|         161.0|\n",
      "|2025-08-31|    X002|  P002|      226|   45238.43|   5654.8|  9948.41|  1303.38|         193.5|\n",
      "|2025-09-01|    X002|  P002|       80|   15406.49|   3081.3|   5835.6|  1475.39|        155.67|\n",
      "|2025-09-02|    X002|  P002|      214|    41073.2|   5867.6| 10071.92|  2118.02|        170.25|\n",
      "|2025-09-03|    X002|  P002|      141|   28619.31|  5723.86|  9367.63|  1433.37|         164.4|\n",
      "+----------+--------+------+---------+-----------+---------+---------+---------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Stock-Out Risk Signal",
   "id": "e0177f14fa8a5dbf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T10:26:35.062475Z",
     "start_time": "2025-09-20T10:26:34.844662Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "inventory_df = inventory_df.withColumn(\"date\", f.to_date(\"timestamp\"))\n",
    "# Join inventory with daily_item_store to get moving_avg_qty for that SKU and store\n",
    "inventory_with_sales = inventory_df.join(\n",
    "    daily_item_store.select(\"date\", \"store_id\", \"sku_id\", \"moving_avg_qty\"),\n",
    "    on=[\"store_id\", \"sku_id\", \"date\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "inventory_with_sales = inventory_with_sales.withColumn(\n",
    "    \"stock_out_signal\",\n",
    "    f.when(f.col(\"moving_avg_qty\").isNotNull() & (f.col(\"on_stock\") <= f.col(\"moving_avg_qty\")), 1)\n",
    "     .otherwise(0)\n",
    ")\n",
    "\n",
    " # Partition to improve joins and window ops\n",
    "inventory_with_sales = inventory_with_sales.repartition(\"store_id\", \"sku_id\")\n",
    "\n",
    "# Streaming DataFrames cannot be cached.\n",
    "\n",
    "# Risk signal will be stored in MongoDB. Write method cannot be used in streaming DataFrames. We will have to use writeStream for this DataFrame\n",
    "\n",
    "# Spark requires checkpoints to keep track of what data has been already processed.\n",
    "\n",
    "# Path for checkpoint and make sure it exists\n",
    "checkpoint_path = Path(project_root / \"checkpoints\" / \"inventory_stock\")\n",
    "checkpoint_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Stores risk signal in MongoDB\n",
    "inventory_with_sales.writeStream \\\n",
    "    .format(\"mongodb\") \\\n",
    "    .option(\"spark.mongodb.output.uri\", mongo_uri) \\\n",
    "    .option(\"database\", mongo_db) \\\n",
    "    .option(\"collection\", \"stock\") \\\n",
    "    .option(\"checkpointLocation\", checkpoint_path) \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .start()\n",
    "\n",
    "\n"
   ],
   "id": "866b6d5a2cb94c2e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/20 20:26:34 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.query.StreamingQuery at 0x7fd442469cf0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Stream mini-stimulation query",
   "id": "69e2872edd63f076"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T10:26:39.239651Z",
     "start_time": "2025-09-20T10:26:35.072342Z"
    }
   },
   "cell_type": "code",
   "source": [
    "console_query = inventory_with_sales.writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", False) \\\n",
    "    .option(\"numRows\", 20) \\\n",
    "    .option(\"checkpointLocation\", str(checkpoint_path / \"console_checkpoint\")) \\\n",
    "    .start()\n",
    "\n",
    "# Stop after 5 batches\n",
    "batch_count = 0\n",
    "while batch_count < 5:\n",
    "    console_query.awaitTermination(1)  # wait 1 sec per loop\n",
    "    batch_count = len(console_query.recentProgress)\n",
    "\n",
    "console_query.stop()\n",
    "print(\"Stopped stream after 5 batches\")\n"
   ],
   "id": "5edfe9a9a45b88a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/20 20:26:35 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "25/09/20 20:26:35 WARN CaseInsensitiveStringMap: Converting duplicated key checkpointLocation into CaseInsensitiveStringMap.\n",
      "25/09/20 20:26:35 WARN CaseInsensitiveStringMap: Converting duplicated key checkpointLocation into CaseInsensitiveStringMap.\n",
      "25/09/20 20:26:35 WARN CaseInsensitiveStringMap: Converting duplicated key checkpointLocation into CaseInsensitiveStringMap.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+--------+------+----------+-------------------+--------+--------------+----------------+\n",
      "|store_id|sku_id|date      |timestamp          |on_stock|moving_avg_qty|stock_out_signal|\n",
      "+--------+------+----------+-------------------+--------+--------------+----------------+\n",
      "|X002    |P002  |2025-08-31|2025-08-31T14:27:13|175     |193.5         |1               |\n",
      "|X002    |P003  |2025-08-31|2025-08-31T22:54:10|205     |199.5         |0               |\n",
      "|X001    |P001  |2025-08-31|2025-08-31T16:51:03|252     |258.0         |1               |\n",
      "|X002    |P001  |2025-08-31|2025-08-31T20:13:28|113     |179.5         |1               |\n",
      "|X001    |P003  |2025-08-31|2025-08-31T19:38:33|212     |112.0         |0               |\n",
      "|X001    |P002  |2025-08-31|2025-08-31T07:07:16|146     |240.5         |1               |\n",
      "+--------+------+----------+-------------------+--------+--------------+----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/20 20:26:36 WARN CaseInsensitiveStringMap: Converting duplicated key checkpointLocation into CaseInsensitiveStringMap.\n",
      "25/09/20 20:26:36 WARN CaseInsensitiveStringMap: Converting duplicated key checkpointLocation into CaseInsensitiveStringMap.\n",
      "25/09/20 20:26:36 WARN CaseInsensitiveStringMap: Converting duplicated key checkpointLocation into CaseInsensitiveStringMap.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+--------+------+----------+-------------------+--------+--------------+----------------+\n",
      "|store_id|sku_id|date      |timestamp          |on_stock|moving_avg_qty|stock_out_signal|\n",
      "+--------+------+----------+-------------------+--------+--------------+----------------+\n",
      "|X002    |P002  |2025-09-04|2025-09-04T02:08:49|203     |177.17        |0               |\n",
      "|X002    |P003  |2025-09-04|2025-09-04T03:40:51|114     |183.83        |1               |\n",
      "|X001    |P001  |2025-09-04|2025-09-04T18:22:08|103     |260.83        |1               |\n",
      "|X002    |P001  |2025-09-04|2025-09-04T15:02:48|288     |232.67        |0               |\n",
      "|X001    |P003  |2025-09-04|2025-09-04T05:06:57|216     |182.17        |0               |\n",
      "|X001    |P002  |2025-09-04|2025-09-04T01:36:11|297     |199.33        |0               |\n",
      "+--------+------+----------+-------------------+--------+--------------+----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/20 20:26:37 WARN CaseInsensitiveStringMap: Converting duplicated key checkpointLocation into CaseInsensitiveStringMap.\n",
      "25/09/20 20:26:37 WARN CaseInsensitiveStringMap: Converting duplicated key checkpointLocation into CaseInsensitiveStringMap.\n",
      "25/09/20 20:26:37 WARN CaseInsensitiveStringMap: Converting duplicated key checkpointLocation into CaseInsensitiveStringMap.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+--------+------+----------+-------------------+--------+--------------+----------------+\n",
      "|store_id|sku_id|date      |timestamp          |on_stock|moving_avg_qty|stock_out_signal|\n",
      "+--------+------+----------+-------------------+--------+--------------+----------------+\n",
      "|X002    |P002  |2025-09-03|2025-09-03T23:48:01|179     |164.4         |0               |\n",
      "|X002    |P003  |2025-09-03|2025-09-03T05:28:57|267     |196.6         |0               |\n",
      "|X001    |P001  |2025-09-03|2025-09-03T03:52:39|178     |283.8         |1               |\n",
      "|X002    |P001  |2025-09-03|2025-09-03T04:08:20|226     |224.2         |0               |\n",
      "|X001    |P003  |2025-09-03|2025-09-03T23:37:36|300     |180.4         |0               |\n",
      "|X001    |P002  |2025-09-03|2025-09-03T04:53:55|169     |201.0         |1               |\n",
      "+--------+------+----------+-------------------+--------+--------------+----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/20 20:26:38 WARN CaseInsensitiveStringMap: Converting duplicated key checkpointLocation into CaseInsensitiveStringMap.\n",
      "25/09/20 20:26:38 WARN CaseInsensitiveStringMap: Converting duplicated key checkpointLocation into CaseInsensitiveStringMap.\n",
      "25/09/20 20:26:38 WARN CaseInsensitiveStringMap: Converting duplicated key checkpointLocation into CaseInsensitiveStringMap.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+--------+------+----------+-------------------+--------+--------------+----------------+\n",
      "|store_id|sku_id|date      |timestamp          |on_stock|moving_avg_qty|stock_out_signal|\n",
      "+--------+------+----------+-------------------+--------+--------------+----------------+\n",
      "|X002    |P002  |2025-09-01|2025-09-01T13:05:37|235     |155.67        |0               |\n",
      "|X002    |P003  |2025-09-01|2025-09-01T11:45:53|299     |231.33        |0               |\n",
      "|X001    |P001  |2025-09-01|2025-09-01T12:00:02|205     |271.33        |1               |\n",
      "|X002    |P001  |2025-09-01|2025-09-01T20:50:38|241     |213.33        |0               |\n",
      "|X001    |P003  |2025-09-01|2025-09-01T03:39:49|198     |157.0         |0               |\n",
      "|X001    |P002  |2025-09-01|2025-09-01T10:38:11|281     |208.0         |0               |\n",
      "+--------+------+----------+-------------------+--------+--------------+----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/20 20:26:38 WARN CaseInsensitiveStringMap: Converting duplicated key checkpointLocation into CaseInsensitiveStringMap.\n",
      "25/09/20 20:26:38 WARN CaseInsensitiveStringMap: Converting duplicated key checkpointLocation into CaseInsensitiveStringMap.\n",
      "25/09/20 20:26:38 WARN CaseInsensitiveStringMap: Converting duplicated key checkpointLocation into CaseInsensitiveStringMap.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 4\n",
      "-------------------------------------------\n",
      "+--------+------+----------+-------------------+--------+--------------+----------------+\n",
      "|store_id|sku_id|date      |timestamp          |on_stock|moving_avg_qty|stock_out_signal|\n",
      "+--------+------+----------+-------------------+--------+--------------+----------------+\n",
      "|X002    |P002  |2025-09-02|2025-09-02T05:15:52|107     |170.25        |1               |\n",
      "|X002    |P003  |2025-09-02|2025-09-02T21:08:38|143     |220.25        |1               |\n",
      "|X001    |P001  |2025-09-02|2025-09-02T16:03:11|261     |274.5         |1               |\n",
      "|X002    |P001  |2025-09-02|2025-09-02T19:41:15|237     |232.75        |0               |\n",
      "|X001    |P003  |2025-09-02|2025-09-02T22:45:06|258     |172.25        |0               |\n",
      "|X001    |P002  |2025-09-02|2025-09-02T22:11:26|144     |185.0         |1               |\n",
      "+--------+------+----------+-------------------+--------+--------------+----------------+\n",
      "\n",
      "Stopped stream after 5 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/20 20:26:39 WARN CaseInsensitiveStringMap: Converting duplicated key checkpointLocation into CaseInsensitiveStringMap.\n",
      "25/09/20 20:26:39 WARN CaseInsensitiveStringMap: Converting duplicated key checkpointLocation into CaseInsensitiveStringMap.\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T10:26:39.316638Z",
     "start_time": "2025-09-20T10:26:39.314366Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "f3b9b3f32bcb18f8",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
